{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import uproot\n",
    "import particle\n",
    "\n",
    "# Imports from this project\n",
    "sys.path.insert(0, \"..\")\n",
    "from utils import paths\n",
    "from utils.input_output import load_data_from_root, load_feature_keys, load_feature_properties\n",
    "from utils.histograms import find_good_binning, get_hist, calc_pull\n",
    "from utils.merge_pdfs import merge_pdfs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constant variables\n",
    "input_files = [paths.B2JpsiKstar_file, paths.Bs2DsPi_file]\n",
    "\n",
    "input_file_keys = [\"DecayTree\", \"Bs2DspiDetached/DecayTree\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_merge_from_root(input_files, input_file_keys, features_to_load, N_events_max_per_dataset=np.Infinity, same_N_events_forced=True):\n",
    "    if same_N_events_forced:\n",
    "        N_events = []\n",
    "        for i, (input_file_path, input_file_key) in enumerate(zip(input_files, input_file_keys)):\n",
    "            with uproot.open(input_file_path)[input_file_key] as tree:\n",
    "                N_events.append(tree.num_entries)\n",
    "\n",
    "        N_events_per_dataset = np.min(N_events + [N_events_max_per_dataset])\n",
    "    else:\n",
    "        N_events_per_dataset = N_events_max_per_dataset\n",
    "    \n",
    "    \n",
    "    # concatenate all DataFrames into one\n",
    "    df = pd.DataFrame()\n",
    "    # iterate over all input files\n",
    "    for i, (input_file_path, input_file_key) in enumerate(tqdm(zip(input_files, input_file_keys), total=len(input_files), desc=\"Datasets\")):\n",
    "        print(N_events_per_dataset)\n",
    "        temp_df = load_data_from_root(input_file_path, \n",
    "                                    tree_key=input_file_key,\n",
    "                                    features=features_to_load, \n",
    "                                    N_entries_max=np.Infinity, \n",
    "                                    batch_size=50000)\n",
    "        \n",
    "        temp_df.rename_axis(index={\"entry\":\"event_id\", \"subentry\": \"track_id\"},  inplace=True)\n",
    "\n",
    "        temp_df[\"input_file_id\"] = i\n",
    "\n",
    "        if \"B2JpsiKstar\" in str(input_file_path):\n",
    "            temp_df[\"decay_id\"] = 0\n",
    "        elif \"Bs2DsPi\" in str(input_file_path):\n",
    "            temp_df[\"decay_id\"] = 1\n",
    "        else:\n",
    "            raise NameError(f\"Decay channel not recognized in Dataset {i}\")\n",
    "\n",
    "        # make sure all event ids are unambiguous\n",
    "        if not df.empty:\n",
    "            temp_df.reset_index(inplace=True)\n",
    "            temp_df[\"event_id\"] += df.index.max()[0] + 1\n",
    "            temp_df.set_index([\"event_id\", \"track_id\"], inplace=True)\n",
    "        \n",
    "        # shuffle the events before shrinking the dataset (to adjust for imbalances)\n",
    "        temp_event_ids = temp_df.index.unique(\"event_id\")\n",
    "        temp_event_ids = np.random.permutation(temp_event_ids)\n",
    "        if N_events_per_dataset == np.Infinity:\n",
    "            temp_df = temp_df.loc[temp_event_ids[:]]\n",
    "        else:\n",
    "            temp_df = temp_df.loc[temp_event_ids[:int(N_events_per_dataset)]]\n",
    "\n",
    "        # append this batch to the DataFrame\n",
    "        df = pd.concat([df, temp_df])\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with uproot.open(paths.B2JpsiKstar_file)[\"DecayTree\"] as tree:\n",
    "    tree.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_load = [\"Tr_T_x\", \"Tr_T_y\", \"Tr_T_Charge\", \"Polarity\"]\n",
    "\n",
    "df = load_and_merge_from_root(input_files, input_file_keys, features_to_load, 1000000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with uproot.open(paths.Bs2DsPi_file, file_handler=uproot.MultithreadedFileSource, num_workers=20)[\"Bs2DspiDetached/DecayTree\"] as tree:\n",
    "    df_Bs = tree.arrays([\"Tr_T_x\", \"Tr_T_y\", \"Tr_T_Charge\", \"Polarity\"], library=\"pd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.query(\"decay_id==0\")[\"Polarity\"].unique(),df.query(\"decay_id==1\")[\"Polarity\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.query(\"decay_id==0\")[\"Polarity\"].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.hist2d()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for B_is_strange in [0,1]:\n",
    "    df_cut = df.query(f\"(Tr_T_Charge==1)&(Polarity==1)&(decay_id=={B_is_strange})\")\n",
    "    \n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.title(f\"B_ID:{B_is_strange}\")\n",
    "    plt.hist2d(df_cut[\"Tr_T_x\"], df_cut[\"Tr_T_y\"], range=[[-250,250],[-250,250]], bins=300)\n",
    "    plt.xlabel(\"Tr_T_x\")\n",
    "    plt.ylabel(\"Tr_T_y\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check for the peak in Tr_p_proj on Bd SS tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_load = [\"B_PX\", \"B_PY\", \"B_PZ\", \"B_PE\", \n",
    "                    \"Tr_T_PX\", \"Tr_T_PY\", \"Tr_T_PZ\", \"Tr_T_E\",\n",
    "                    \"B_TRUEID\", \"Tr_ORIG_FLAGS\",\n",
    "                    \"Tr_MC_ID\", \"Tr_MC_MOTHER_ID\"]\n",
    "\n",
    "df = load_and_merge_from_root(input_files, input_file_keys, features_to_load, 1000000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Tr_is_SS\"] = (df[\"Tr_ORIG_FLAGS\"] == 1).astype(int)\n",
    "assert set(df[\"B_TRUEID\"].unique()) == set([511,-511,531,-531]), \"There are other signal particles than B0 and Bs\"\n",
    "df[\"B_is_strange\"] = (np.abs(df[\"B_TRUEID\"]) == 531).astype(int)\n",
    "\n",
    "PX_proj = -1 * df[f\"B_PX\"] * df[f\"Tr_T_PX\"]\n",
    "PY_proj = -1 * df[f\"B_PY\"] * df[f\"Tr_T_PY\"]\n",
    "PZ_proj = -1 * df[f\"B_PZ\"] * df[f\"Tr_T_PZ\"]\n",
    "PE_proj = +1 * df[f\"B_PE\"] * df[f\"Tr_T_E\"]\n",
    "\n",
    "df[\"Tr_p_proj\"] = np.sum([PX_proj, PY_proj, PZ_proj, PE_proj], axis=0)\n",
    "df[\"log10_Tr_p_proj\"] = np.log10(df[\"Tr_p_proj\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ss = df.query(\"Tr_is_SS == 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tr_p_proj by Tr_MC_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = []\n",
    "y = []\n",
    "uniq_mc_id, counts_mc_id = np.unique(df_ss[\"Tr_MC_ID\"], return_counts=True)\n",
    "sort_mask = np.argsort(-counts_mc_id)\n",
    "uniq_mc_id = uniq_mc_id[sort_mask]\n",
    "for mc_id in uniq_mc_id:\n",
    "    df_mc_id = df_ss.query(f\"Tr_MC_ID == {mc_id}\")\n",
    "    \n",
    "    x.append(df_mc_id.query(\"B_is_strange == 0\")[\"log10_Tr_p_proj\"])\n",
    "    y.append(df_mc_id.query(\"B_is_strange == 1\")[\"log10_Tr_p_proj\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from particle import Particle\n",
    "particles = []\n",
    "for mc_id in tqdm(uniq_mc_id):\n",
    "    try:\n",
    "        p = Particle.from_pdgid(mc_id)\n",
    "        particles.append(p.name)\n",
    "    except particle.ParticleNotFound as ex:\n",
    "        particles.append(mc_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,2, figsize=(20,10), sharey=True, sharex=True)\n",
    "fig.suptitle(\"log10(Tr_p_proj) by Tr_MC_ID (only SS tracks selected)\")\n",
    "\n",
    "axs[0].hist(x,bins=100,range=(6.0,7.0), stacked=True, label=particles)\n",
    "axs[0].set_title(\"Bd\")\n",
    "axs[0].set_ylabel(\"counts\")\n",
    "axs[0].set_xlabel(\"log10(Tr_p_proj)\")\n",
    "\n",
    "axs[1].hist(y,bins=100,range=(6.0,7.0), stacked=True, label=particles)\n",
    "axs[1].set_title(\"Bs\")\n",
    "axs[1].set_ylabel(\"counts\")\n",
    "axs[1].set_xlabel(\"log10_Tr_p_proj\")\n",
    "\n",
    "# reversed legend\n",
    "handles, labels = axs[0].get_legend_handles_labels()\n",
    "axs[0].legend(handles[::-1], labels[::-1], title='Particle', loc='upper left')\n",
    "\n",
    "handles, labels = axs[1].get_legend_handles_labels()\n",
    "axs[1].legend(handles[::-1], labels[::-1], title='Particle', loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tr_p_proj by Tr_MC_MOTHER_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = []\n",
    "y = []\n",
    "uniq_mc_id, counts_mc_id = np.unique(df_ss[\"Tr_MC_MOTHER_ID\"], return_counts=True)\n",
    "sort_mask = np.argsort(-counts_mc_id)\n",
    "uniq_mc_id = uniq_mc_id[sort_mask]\n",
    "for mc_id in tqdm(uniq_mc_id):\n",
    "    df_mc_id = df_ss.query(f\"Tr_MC_MOTHER_ID == {mc_id}\")\n",
    "    \n",
    "    x.append(df_mc_id.query(\"B_is_strange == 0\")[\"log10_Tr_p_proj\"])\n",
    "    y.append(df_mc_id.query(\"B_is_strange == 1\")[\"log10_Tr_p_proj\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from particle import Particle\n",
    "particles = []\n",
    "for mc_id in uniq_mc_id:\n",
    "    try:\n",
    "        p = Particle.from_pdgid(mc_id)\n",
    "        particles.append(p.name)\n",
    "    except particle.ParticleNotFound as ex:\n",
    "        particles.append(mc_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,2, figsize=(20,10), sharey=True, sharex=True)\n",
    "fig.suptitle(\"log10(Tr_p_proj) by Tr_MC_MOTHER_ID (only SS tracks selected)\")\n",
    "\n",
    "axs[0].hist(x,bins=100,range=(6.0,7.0), stacked=True, label=particles)\n",
    "axs[0].set_title(\"Bd\")\n",
    "axs[0].set_ylabel(\"counts\")\n",
    "axs[0].set_xlabel(\"log10(Tr_p_proj)\")\n",
    "\n",
    "axs[1].hist(y,bins=100,range=(6.0,7.0), stacked=True, label=particles)\n",
    "axs[1].set_title(\"Bs\")\n",
    "axs[1].set_ylabel(\"counts\")\n",
    "axs[1].set_xlabel(\"log10(Tr_p_proj)\")\n",
    "\n",
    "# reversed legend\n",
    "#handles, labels = axs[0].get_legend_handles_labels()\n",
    "#axs[0].legend(handles[::-1], labels[::-1], title='Particle', loc='upper left')\n",
    "#\n",
    "#handles, labels = axs[1].get_legend_handles_labels()\n",
    "#axs[1].legend(handles[::-1], labels[::-1], title='Particle', loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tr_p_proj by Tr_MC_MOTHER_ID zoomed in to see what are the resonances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log10p_min = 6.32\n",
    "log10p_max = 6.42\n",
    "\n",
    "temp_df = df_ss.query(f\"{log10p_min} <= log10_Tr_p_proj <= {log10p_max}\")\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "uniq_mc_id, counts_mc_id = np.unique(temp_df[\"Tr_MC_MOTHER_ID\"], return_counts=True)\n",
    "sort_mask = np.argsort(-counts_mc_id)\n",
    "uniq_mc_id = uniq_mc_id[sort_mask]\n",
    "for mc_id in tqdm(uniq_mc_id):\n",
    "    df_mc_id = temp_df.query(f\"Tr_MC_MOTHER_ID == {mc_id}\")\n",
    "    \n",
    "    x.append(df_mc_id.query(\"B_is_strange == 0\")[\"log10_Tr_p_proj\"])\n",
    "    y.append(df_mc_id.query(\"B_is_strange == 1\")[\"log10_Tr_p_proj\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort by the difference in max to min bin counts\n",
    "x_hists = []\n",
    "x_hist_diffs = []\n",
    "for xi in x:\n",
    "    x_hist, bin_edges = np.histogram(xi, bins=100, range=(log10p_min,log10p_max))\n",
    "    x_hists.append(x_hist)\n",
    "    \n",
    "    x_hist_diff = np.max(x_hist) - np.min(x_hist)\n",
    "    x_hist_diffs.append(x_hist_diff)\n",
    "    \n",
    "x_hist_diffs = np.array(x_hist_diffs)    \n",
    "sort_mask = np.argsort(-x_hist_diffs)\n",
    "\n",
    "uniq_mc_id = uniq_mc_id[sort_mask]\n",
    "x = [x[i] for i in sort_mask]\n",
    "y = [y[i] for i in sort_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from particle import Particle\n",
    "particles = []\n",
    "for mc_id in uniq_mc_id:\n",
    "    try:\n",
    "        p = Particle.from_pdgid(mc_id)\n",
    "        particles.append(p.name)\n",
    "    except particle.ParticleNotFound as ex:\n",
    "        particles.append(mc_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,2, figsize=(20,10), sharey=True, sharex=True)\n",
    "fig.suptitle(\"log10(Tr_p_proj) by Tr_MC_MOTHER_ID (only SS tracks selected)\")\n",
    "\n",
    "axs[0].hist(x,bins=100,range=(log10p_min,log10p_max), stacked=True, label=particles)\n",
    "axs[0].set_title(\"Bd\")\n",
    "axs[0].set_ylabel(\"counts\")\n",
    "axs[0].set_xlabel(\"log10(Tr_p_proj)\")\n",
    "\n",
    "axs[1].hist(y,bins=100,range=(log10p_min,log10p_max), stacked=True, label=particles)\n",
    "axs[1].set_title(\"Bs\")\n",
    "axs[1].set_ylabel(\"counts\")\n",
    "axs[1].set_xlabel(\"log10(Tr_p_proj)\")\n",
    "\n",
    "# reversed legend\n",
    "handles, labels = axs[0].get_legend_handles_labels()\n",
    "axs[0].legend(handles[10::-1], labels[10::-1], title='Particle', loc='upper left')\n",
    "\n",
    "handles, labels = axs[1].get_legend_handles_labels()\n",
    "axs[1].legend(handles[10::-1], labels[10::-1], title='Particle', loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check for correlation to the B mass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_load = load_feature_keys([\"direct\"]) + [\"B_M\"]\n",
    "\n",
    "df = load_and_merge_from_root(input_files, input_file_keys, features_to_load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_keys = load_feature_keys([\"direct\"])\n",
    "label_key = \"B_M\"\n",
    "\n",
    "\n",
    "# Read in the feature properties\n",
    "fprops = load_feature_properties()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Apply cuts to the data\n",
    "lower_quantile = 0.0001\n",
    "higher_quantile = 0.9999\n",
    "\n",
    "cut_loss = {}\n",
    "\n",
    "mask = True\n",
    "for feature in tqdm(feature_keys, desc=\"Apply Feature Cuts\"):\n",
    "    if fprops[feature][\"feature_type\"] == \"numerical\":\n",
    "        temp_mask = fprops[feature][f\"quantile_{lower_quantile}\"] <= df[feature]\n",
    "        temp_mask &= df[feature] <= fprops[feature][f\"quantile_{higher_quantile}\"]\n",
    "        \n",
    "        # include the error value because else, to much tracks get lost\n",
    "        if \"error_value\" in fprops[feature].keys():\n",
    "            temp_mask |= df[feature] == fprops[feature][\"error_value\"]\n",
    "        \n",
    "        cut_loss[feature] = {}\n",
    "        cut_loss[feature][\"relative_loss\"] = (~temp_mask).sum()/len(temp_mask)\n",
    "        cut_loss[feature][\"absolute_loss\"] = (~temp_mask).sum()\n",
    "        \n",
    "        mask &= temp_mask\n",
    "        \n",
    "df_data_cut = df[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Rearrange the features so that categorical features come last\n",
    "numerical_features = [feature for feature in feature_keys if fprops[feature][\"feature_type\"] == \"numerical\"]\n",
    "categorical_features = [feature for feature in feature_keys if fprops[feature][\"feature_type\"] == \"categorical\"]\n",
    "\n",
    "feature_keys = numerical_features + categorical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% \n",
    "# Calculate the correlation matrix\n",
    "print(\"Calculating the correlation matrix...\")\n",
    "df_corr = df_data_cut[feature_keys+[label_key]].corr()\n",
    "print(\"Done calculating the correlation matrix.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr[f\"abs_{label_key}\"] = np.abs(df_corr[label_key])\n",
    "\n",
    "df_corr.sort_values(by=f\"abs_{label_key}\", ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the correlation to B_M\n",
    "fig, ax = plt.subplots(1,1, figsize=(len(features_to_load)*1, 8))\n",
    "\n",
    "#fig.suptitle(f\"Feature Importance\")\n",
    "\n",
    "ax.set_title(f\"Correlation to {label_key}\")\n",
    "\n",
    "ax.bar(df_corr.index, np.abs(df_corr.loc[:,label_key]), alpha=0.8, zorder=3)\n",
    "\n",
    "ax.set_xlabel(\"Feature\")\n",
    "ax.set_ylabel(f\"abs(Correlation to {label_key})\")\n",
    "\n",
    "ax.tick_params(axis=\"y\", left=True, right=True, labelleft=True, labelright=True)\n",
    "ax.tick_params(axis=\"x\", bottom=True, top=True, rotation=45)\n",
    "ax.grid(zorder=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(paths.plots_dir/f\"corr_to_{label_key}.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "20b05ee88e26e7f23d90b869a85a4fb598ebc8861a40413c4d0961bc2f50b067"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('root_forge')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
